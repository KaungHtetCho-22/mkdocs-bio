{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biodiversity \u00b6 Welcome to the Biodiversity Project Documentation . This guide provides detailed instructions, references, and technical explanations for setting up the hardware, training AI models, running inference pipelines, and displaying results on the web. 1. Hardware Components \u00b6 This section explains how to set up the hardware used in the project and configure the necessary scripts to start operations. Step-by-step installation and configuration Overview of IoT devices and their roles Key scripts and commands for operation Read more \u2192 2. AI Models \u00b6 This section describes the AI models developed for biodiversity monitoring, including sound classification and score prediction . 2.1. Overview Procedure \u00b6 General workflow for model training and evaluation. 2.2. Sound Classification / Training \u00b6 Data preparation and preprocessing Model architecture and training parameters Evaluation metrics 2.3. Score Prediction / Training \u00b6 Feature engineering Model training and validation Performance monitoring Read more \u2192 3. Inference \u00b6 This section explains how the system performs inference and serves results. 3.1. Overview Procedure \u00b6 High-level inference workflow. 3.2. Database Design \u00b6 Schema design and data storage strategy. 3.3. Inference Pipeline Design \u00b6 Step-by-step data flow for predictions. 3.4. Results Showing on the Web \u00b6 Web UI integration for visualizing predictions and analytics. Read more \u2192 About This Documentation \u00b6 This documentation is part of the Biodiversity Project , which aims to automate biodiversity monitoring using IoT devices, AI-driven sound classification, and real-time score predictions. It is organized for developers, researchers, and field operators to follow consistently from setup to deployment. All the source code are pushed to the github","title":"Home"},{"location":"#biodiversity","text":"Welcome to the Biodiversity Project Documentation . This guide provides detailed instructions, references, and technical explanations for setting up the hardware, training AI models, running inference pipelines, and displaying results on the web.","title":"Biodiversity"},{"location":"#1-hardware-components","text":"This section explains how to set up the hardware used in the project and configure the necessary scripts to start operations. Step-by-step installation and configuration Overview of IoT devices and their roles Key scripts and commands for operation Read more \u2192","title":"1. Hardware Components"},{"location":"#2-ai-models","text":"This section describes the AI models developed for biodiversity monitoring, including sound classification and score prediction .","title":"2. AI Models"},{"location":"#21-overview-procedure","text":"General workflow for model training and evaluation.","title":"2.1. Overview Procedure"},{"location":"#22-sound-classification-training","text":"Data preparation and preprocessing Model architecture and training parameters Evaluation metrics","title":"2.2. Sound Classification / Training"},{"location":"#23-score-prediction-training","text":"Feature engineering Model training and validation Performance monitoring Read more \u2192","title":"2.3. Score Prediction / Training"},{"location":"#3-inference","text":"This section explains how the system performs inference and serves results.","title":"3. Inference"},{"location":"#31-overview-procedure","text":"High-level inference workflow.","title":"3.1. Overview Procedure"},{"location":"#32-database-design","text":"Schema design and data storage strategy.","title":"3.2. Database Design"},{"location":"#33-inference-pipeline-design","text":"Step-by-step data flow for predictions.","title":"3.3. Inference Pipeline Design"},{"location":"#34-results-showing-on-the-web","text":"Web UI integration for visualizing predictions and analytics. Read more \u2192","title":"3.4. Results Showing on the Web"},{"location":"#about-this-documentation","text":"This documentation is part of the Biodiversity Project , which aims to automate biodiversity monitoring using IoT devices, AI-driven sound classification, and real-time score predictions. It is organized for developers, researchers, and field operators to follow consistently from setup to deployment. All the source code are pushed to the github","title":"About This Documentation"},{"location":"ai/","text":"AI Models \u00b6 The AI module in the Biodiversity Project is composed of two main parts: Sound Classification System Identifies bird and insect species, as well as noise categories, from recorded audio. Score Prediction Model Generates a biodiversity score based on the outputs of the sound classification system. Sound Model Training \u00b6 This section describes the process and details for training the sound classification model. 1. Training Dataset \u00b6 The dataset consists of bird species , insect species , and noise classes . 1.1 Bird Species \u00b6 Common Name Biological Name Number of Samples Asian Koel Eudynamys scolopaceus 1200 (Add more rows here) 1.2 Insect Species \u00b6 Common Name Biological Name Number of Samples Cicada Cicadidae 800 (Add more rows here) 1.3 Noise Classes \u00b6 Class Name Description Number of Samples (Example) Rain Background rain noise 500 (Add more rows here) 2. Model Accuracy \u00b6 Metric Value Accuracy XX% Precision XX% Recall XX% F1-Score XX% (Replace XX% with actual results after training.) 3. Model Architecture \u00b6 The sound classification system is based on: Input: Mel-spectrograms extracted from audio recordings. Feature Extraction: Convolutional Neural Networks (CNNs) for spatial feature learning. Classification Layer: Fully-connected layers with softmax output for multi-class classification. Training Details: Optimizer: Adam Loss: Categorical Cross-Entropy Learning Rate: (e.g., 0.001) Epochs: (e.g., 50) Batch Size: (e.g., 32) Next: Score Prediction Model","title":"AI Models"},{"location":"ai/#ai-models","text":"The AI module in the Biodiversity Project is composed of two main parts: Sound Classification System Identifies bird and insect species, as well as noise categories, from recorded audio. Score Prediction Model Generates a biodiversity score based on the outputs of the sound classification system.","title":"AI Models"},{"location":"ai/#sound-model-training","text":"This section describes the process and details for training the sound classification model.","title":"Sound Model Training"},{"location":"ai/#1-training-dataset","text":"The dataset consists of bird species , insect species , and noise classes .","title":"1. Training Dataset"},{"location":"ai/#11-bird-species","text":"Common Name Biological Name Number of Samples Asian Koel Eudynamys scolopaceus 1200 (Add more rows here)","title":"1.1 Bird Species"},{"location":"ai/#12-insect-species","text":"Common Name Biological Name Number of Samples Cicada Cicadidae 800 (Add more rows here)","title":"1.2 Insect Species"},{"location":"ai/#13-noise-classes","text":"Class Name Description Number of Samples (Example) Rain Background rain noise 500 (Add more rows here)","title":"1.3 Noise Classes"},{"location":"ai/#2-model-accuracy","text":"Metric Value Accuracy XX% Precision XX% Recall XX% F1-Score XX% (Replace XX% with actual results after training.)","title":"2. Model Accuracy"},{"location":"ai/#3-model-architecture","text":"The sound classification system is based on: Input: Mel-spectrograms extracted from audio recordings. Feature Extraction: Convolutional Neural Networks (CNNs) for spatial feature learning. Classification Layer: Fully-connected layers with softmax output for multi-class classification. Training Details: Optimizer: Adam Loss: Categorical Cross-Entropy Learning Rate: (e.g., 0.001) Epochs: (e.g., 50) Batch Size: (e.g., 32) Next: Score Prediction Model","title":"3. Model Architecture"},{"location":"data/","text":"This is the data augmentation documentation \u00b6","title":"Data Documentation"},{"location":"data/#this-is-the-data-augmentation-documentation","text":"","title":"This is the data augmentation documentation"},{"location":"hardware/","text":"Hardware Components & Setup Documentation \u00b6 1. Device List \u00b6 This system consists of the following IoT hardware components: Raspberry Pi 3 B+ (64 GB microSD storage) AudioMoth (Acoustic logger) 4G Router (with SIM card) Internet SIM Card Solar Panel (with battery storage) 2. Raspberry Pi Setup \u00b6 2.1. Hardware \u00b6 Model: Raspberry Pi 3 B+ Storage: 64 GB microSD card 2.2. OS Installation \u00b6 Insert the SD card into your computer. Use Balena Etcher (or similar) to flash the provided .img backup OS image. Insert the flashed SD card into the Raspberry Pi. Power on the device. Raspberry Pi SD Card \ud83d\udce5 Download Raspberry Pi OS Image 2.3. Login Credentials \u00b6 Default credentials (can be customized): - Username: pi - Password: raspberry 3. VPN Configuration \u00b6 Each Raspberry Pi has its own OpenVPN account. 3.1. File Structure \u00b6 Navigate to the OpenVPN directory: cd /etc/openvpn/ ls Expected files: client/ credentials.txt openvpn_MONSOON_TEA05.conf server/ update-resolv-conf openvpn_MONSOON_TEA05.conf \u2192 Converted .ovpn client config file credentials.txt \u2192 VPN username & password (two lines only) 3.2. Credentials Setup \u00b6 Check credentials: cat /etc/openvpn/credentials.txt Format: vpn_username vpn_password Secure file permissions: sudo chmod 600 /etc/openvpn/credentials.txt 3.3. VPN Service Setup \u00b6 Enable & start service: sudo systemctl enable openvpn@openvpn_MONSOON_TEA05 sudo systemctl start openvpn@openvpn_MONSOON_TEA05 Manual connect: sudo openvpn --config openvpn_MONSOON_TEA05.ovpn Verify connection: ifconfig VPN tunnel should point to 10.81.234.5 . 4. Device Configuration File \u00b6 Example config.json : { \"ftp\" : { \"uname\" : \"monsoon\" , \"pword\" : \"p8z3%1P#04\" , \"host\" : \"192.168.70.5/production-workflow-ec2\" , \"use_ftps\" : 1 }, \"offline_mode\" : 0 , \"sensor\" : { \"sensor_index\" : 2 , \"sensor_type\" : \"USBSoundcardMic\" , \"record_length\" : 600 , \"compress_data\" : false , \"capture_delay\" : 0 }, \"sys\" : { \"working_dir\" : \"/home/pi/tmp_dir\" , \"upload_dir\" : \"/home/pi/continuous_monitoring_data\" , \"reboot_time\" : \"02:00\" }, \"device_id\" : \"00000000f1c084c2\" } 5. Automatic Recording Service \u00b6 Example systemd service ( /etc/systemd/system/shellscript.service ): [Unit] Description = My Shell Script [Service] ExecStart = /home/pi/custom-pi-setup/recorder_startup_script.sh [Install] WantedBy = multi-user.target Check live service logs: journalctl -u shellscript.service -f 6. Important Commands \u00b6 Command Purpose arecord -l List available recording devices journalctl -u shellscript.service -f Live monitoring of recording service sudo systemctl restart shellscript.service Restart recording service 7. AudioMoth Setup \u00b6 7.1. Overview \u00b6 AudioMoth is a low-cost, full-spectrum acoustic logger, based on the Gecko processor range from Silicon Labs. It can record audible and ultrasonic frequencies at rates from 8,000 to 384,000 samples/sec . It is used in two modes: mobile and station . 7.2. Modes \u00b6 Mobile Type \u00b6 Portable configuration for temporary deployments Ideal for short-term surveys \ud83d\udcc4 Download Mobile AudioMoth Manual (PDF) Station Type \u00b6 Fixed position setup for continuous monitoring Powered by solar & external battery \ud83d\udcc4 Download IoT Station Setup Manual (PDF) 8. Router Setup & Troubleshooting \u00b6 Type: 4G Router with SIM Purpose: Internet connection for remote locations Troubleshooting Checklist: 1. Check LED status indicators 2. Ensure SIM card is active 3. Restart router if connection drops 9. Solar Panel & Battery \u00b6 Solar Panel \u00b6 Powers IoT devices in remote areas Indicators: Green \u2192 Charging Red \u2192 Low battery Off \u2192 No power Battery \u00b6 Stores energy for night/cloudy use Blink Indicators: 1 blink \u2192 Low 2 blinks \u2192 Medium 3 blinks \u2192 Full 10. System Workflow \u00b6 Power Supply \u2192 Solar Panel \u2192 Battery \u2192 Raspberry Pi & Router Data Capture \u2192 AudioMoth or Raspberry Pi records audio Data Transmission \u2192 Router sends via 4G Remote Access \u2192 VPN connection for management Monitoring \u2192 Logs checked via journalctl or SSH","title":"Hardware"},{"location":"hardware/#hardware-components-setup-documentation","text":"","title":"Hardware Components &amp; Setup Documentation"},{"location":"hardware/#1-device-list","text":"This system consists of the following IoT hardware components: Raspberry Pi 3 B+ (64 GB microSD storage) AudioMoth (Acoustic logger) 4G Router (with SIM card) Internet SIM Card Solar Panel (with battery storage)","title":"1. Device List"},{"location":"hardware/#2-raspberry-pi-setup","text":"","title":"2. Raspberry Pi Setup"},{"location":"hardware/#21-hardware","text":"Model: Raspberry Pi 3 B+ Storage: 64 GB microSD card","title":"2.1. Hardware"},{"location":"hardware/#22-os-installation","text":"Insert the SD card into your computer. Use Balena Etcher (or similar) to flash the provided .img backup OS image. Insert the flashed SD card into the Raspberry Pi. Power on the device. Raspberry Pi SD Card \ud83d\udce5 Download Raspberry Pi OS Image","title":"2.2. OS Installation"},{"location":"hardware/#23-login-credentials","text":"Default credentials (can be customized): - Username: pi - Password: raspberry","title":"2.3. Login Credentials"},{"location":"hardware/#3-vpn-configuration","text":"Each Raspberry Pi has its own OpenVPN account.","title":"3. VPN Configuration"},{"location":"hardware/#31-file-structure","text":"Navigate to the OpenVPN directory: cd /etc/openvpn/ ls Expected files: client/ credentials.txt openvpn_MONSOON_TEA05.conf server/ update-resolv-conf openvpn_MONSOON_TEA05.conf \u2192 Converted .ovpn client config file credentials.txt \u2192 VPN username & password (two lines only)","title":"3.1. File Structure"},{"location":"hardware/#32-credentials-setup","text":"Check credentials: cat /etc/openvpn/credentials.txt Format: vpn_username vpn_password Secure file permissions: sudo chmod 600 /etc/openvpn/credentials.txt","title":"3.2. Credentials Setup"},{"location":"hardware/#33-vpn-service-setup","text":"Enable & start service: sudo systemctl enable openvpn@openvpn_MONSOON_TEA05 sudo systemctl start openvpn@openvpn_MONSOON_TEA05 Manual connect: sudo openvpn --config openvpn_MONSOON_TEA05.ovpn Verify connection: ifconfig VPN tunnel should point to 10.81.234.5 .","title":"3.3. VPN Service Setup"},{"location":"hardware/#4-device-configuration-file","text":"Example config.json : { \"ftp\" : { \"uname\" : \"monsoon\" , \"pword\" : \"p8z3%1P#04\" , \"host\" : \"192.168.70.5/production-workflow-ec2\" , \"use_ftps\" : 1 }, \"offline_mode\" : 0 , \"sensor\" : { \"sensor_index\" : 2 , \"sensor_type\" : \"USBSoundcardMic\" , \"record_length\" : 600 , \"compress_data\" : false , \"capture_delay\" : 0 }, \"sys\" : { \"working_dir\" : \"/home/pi/tmp_dir\" , \"upload_dir\" : \"/home/pi/continuous_monitoring_data\" , \"reboot_time\" : \"02:00\" }, \"device_id\" : \"00000000f1c084c2\" }","title":"4. Device Configuration File"},{"location":"hardware/#5-automatic-recording-service","text":"Example systemd service ( /etc/systemd/system/shellscript.service ): [Unit] Description = My Shell Script [Service] ExecStart = /home/pi/custom-pi-setup/recorder_startup_script.sh [Install] WantedBy = multi-user.target Check live service logs: journalctl -u shellscript.service -f","title":"5. Automatic Recording Service"},{"location":"hardware/#6-important-commands","text":"Command Purpose arecord -l List available recording devices journalctl -u shellscript.service -f Live monitoring of recording service sudo systemctl restart shellscript.service Restart recording service","title":"6. Important Commands"},{"location":"hardware/#7-audiomoth-setup","text":"","title":"7. AudioMoth Setup"},{"location":"hardware/#71-overview","text":"AudioMoth is a low-cost, full-spectrum acoustic logger, based on the Gecko processor range from Silicon Labs. It can record audible and ultrasonic frequencies at rates from 8,000 to 384,000 samples/sec . It is used in two modes: mobile and station .","title":"7.1. Overview"},{"location":"hardware/#72-modes","text":"","title":"7.2. Modes"},{"location":"hardware/#mobile-type","text":"Portable configuration for temporary deployments Ideal for short-term surveys \ud83d\udcc4 Download Mobile AudioMoth Manual (PDF)","title":"Mobile Type"},{"location":"hardware/#station-type","text":"Fixed position setup for continuous monitoring Powered by solar & external battery \ud83d\udcc4 Download IoT Station Setup Manual (PDF)","title":"Station Type"},{"location":"hardware/#8-router-setup-troubleshooting","text":"Type: 4G Router with SIM Purpose: Internet connection for remote locations Troubleshooting Checklist: 1. Check LED status indicators 2. Ensure SIM card is active 3. Restart router if connection drops","title":"8. Router Setup &amp; Troubleshooting"},{"location":"hardware/#9-solar-panel-battery","text":"","title":"9. Solar Panel &amp; Battery"},{"location":"hardware/#solar-panel","text":"Powers IoT devices in remote areas Indicators: Green \u2192 Charging Red \u2192 Low battery Off \u2192 No power","title":"Solar Panel"},{"location":"hardware/#battery","text":"Stores energy for night/cloudy use Blink Indicators: 1 blink \u2192 Low 2 blinks \u2192 Medium 3 blinks \u2192 Full","title":"Battery"},{"location":"hardware/#10-system-workflow","text":"Power Supply \u2192 Solar Panel \u2192 Battery \u2192 Raspberry Pi & Router Data Capture \u2192 AudioMoth or Raspberry Pi records audio Data Transmission \u2192 Router sends via 4G Remote Access \u2192 VPN connection for management Monitoring \u2192 Logs checked via journalctl or SSH","title":"10. System Workflow"},{"location":"inference/","text":"Biodiversity Audio Inference Pipeline \u00b6 A real-time audio monitoring and species detection system for biodiversity research using deep learning models and Raspberry Pi devices. Overview \u00b6 This pipeline processes audio recordings from field-deployed Raspberry Pi devices to automatically detect and classify bird species and other wildlife sounds. It uses a pre-trained deep learning model to analyze audio spectrograms and identify species with confidence scores. Architecture \u00b6 Core Components \u00b6 Audio Processing : Real-time audio file monitoring and processing using librosa Deep Learning Model : Attention-based neural network for species classification Database : SQLite database for storing detections, audio metadata, and device information File Monitoring : Watchdog-based file system monitoring for automatic processing Docker Support : Containerized deployment for development and production Model Architecture \u00b6 The system uses an AttModel class that combines: - Backbone : Pre-trained vision models (configurable via timm) - Mel-spectrogram extraction : Audio preprocessing with configurable parameters - Attention mechanisms : For temporal and frequency domain feature learning - Classification head : Multi-class species prediction with confidence scoring Project Structure \u00b6 inference - pipeline / \u251c\u2500\u2500 app - data / # Database files \u251c\u2500\u2500 audio - data / # Input audio recordings \u251c\u2500\u2500 docker / # Docker configuration files \u251c\u2500\u2500 json - output / # Prediction results and reports \u251c\u2500\u2500 logs / # Application logs \u251c\u2500\u2500 monsoon_biodiversity_common / # Core library modules \u2502 \u251c\u2500\u2500 config . py # Model and system configuration \u2502 \u251c\u2500\u2500 dataset . py # Data loading and preprocessing \u2502 \u251c\u2500\u2500 db_model . py # Database models and schema \u2502 \u251c\u2500\u2500 model . py # Neural network architecture \u2502 \u2514\u2500\u2500 requirements . txt # Core dependencies \u251c\u2500\u2500 scripts / # Utility and deployment scripts \u251c\u2500\u2500 src / # Main application source code \u2502 \u251c\u2500\u2500 debug_audio_monitoring . py # Real-time audio monitoring \u2502 \u251c\u2500\u2500 debug_process_detections . py # Detection processing and reporting \u2502 \u251c\u2500\u2500 inference_station . py # Station-specific inference \u2502 \u251c\u2500\u2500 query . py # Database query utilities \u2502 \u2514\u2500\u2500 species_mapping . py # Species classification mapping \u251c\u2500\u2500 weights / # Pre-trained model weights \u2514\u2500\u2500 requirements . txt # Main project dependencies Quick Start \u00b6 Prerequisites \u00b6 Python 3.8+ Docker (optional, for containerized deployment) Audio processing libraries (librosa, torchaudio) Deep learning framework (PyTorch) Installation \u00b6 Clone the repository This one is already clone at iNET (server4 machine) bash git clone <repository-url> cd inference-pipeline git submodule update --init --recursive Download model weights The weights are already uploaded to the iNET and attached to the drive as well Place soundscape-model.pt file in the weights/ directory sound-scape.pt = sound classification model xgboost-model.pkl = score prediction model Ensure the model architecture matches the configuration in monsoon_biodiversity_common/config.py Setup database bash # The database will be automatically initialized on first run python src/debug_audio_monitoring.py Docker Deployment \u00b6 Build production image bash ./scripts/build_prod_image.sh Run with docker-compose bash cd docker docker-compose -f docker-compose-devel.yaml up -d Access container bash docker exec -it dev-bio-diversity bash Run script bash sh run.sh it will run the audio-monitoring.py and process_detection.py and the intention of those python script will be explained below session Configuration \u00b6 Database \u00b6 This is how the conceptual diagram works inside the inference data accepting Model Configuration \u00b6 Edit monsoon_biodiversity_common/config.py to customize: Audio parameters : Sample rate, mel bands, FFT settings Model architecture : Backbone model, number of classes Training settings : Learning rate, batch size, epochs Usage \u00b6 Real-time Audio Monitoring \u00b6 Start the audio monitoring service: python src/audio_monitoring.py This service: - Monitors audio directories for new files - Processes audio files through the species detection model - Stores results in the database - Generates real-time logs Batch Processing \u00b6 Process existing audio files: python src/inference_station.py Query Results \u00b6 Query detection results from the database: python src/query.py Daily Reports \u00b6 Generate daily detection summaries: python src/debug_process_detections.py --schedule Database Schema \u00b6 Core Tables \u00b6 RpiDevices : Device information and metadata AudioFiles : Audio file records and metadata SpeciesDetections : Detection results with confidence scores Query Examples \u00b6 from src.query import query_species_by_device_and_dates # Query detections for specific device and dates results = query_species_by_device_and_dates ( engine , \"RPiID-0000000081519079\" , [ \"2025-04-23\" , \"2025-04-22\" ] ) Output Formats \u00b6 Detection Results \u00b6 Species detections are stored with: - Audio file reference - Species classification - Confidence score - Temporal segment information - Device and timestamp metadata Log Files \u00b6 audio_inference.log : Real-time processing logs batch_audio_inference.log : Batch processing logs daily_report.log : Daily summary reports Troubleshooting \u00b6 Common Issues \u00b6 Model weights not found Ensure soundscape-model.pt is in the weights/ directory Check file permissions and paths Audio directory not accessible Verify audio data directory exists and is readable Check Docker volume mounts if using containers Database connection errors Ensure SQLite database directory is writable Check database file permissions","title":"Inference"},{"location":"inference/#biodiversity-audio-inference-pipeline","text":"A real-time audio monitoring and species detection system for biodiversity research using deep learning models and Raspberry Pi devices.","title":"Biodiversity Audio Inference Pipeline"},{"location":"inference/#overview","text":"This pipeline processes audio recordings from field-deployed Raspberry Pi devices to automatically detect and classify bird species and other wildlife sounds. It uses a pre-trained deep learning model to analyze audio spectrograms and identify species with confidence scores.","title":"Overview"},{"location":"inference/#architecture","text":"","title":"Architecture"},{"location":"inference/#core-components","text":"Audio Processing : Real-time audio file monitoring and processing using librosa Deep Learning Model : Attention-based neural network for species classification Database : SQLite database for storing detections, audio metadata, and device information File Monitoring : Watchdog-based file system monitoring for automatic processing Docker Support : Containerized deployment for development and production","title":"Core Components"},{"location":"inference/#model-architecture","text":"The system uses an AttModel class that combines: - Backbone : Pre-trained vision models (configurable via timm) - Mel-spectrogram extraction : Audio preprocessing with configurable parameters - Attention mechanisms : For temporal and frequency domain feature learning - Classification head : Multi-class species prediction with confidence scoring","title":"Model Architecture"},{"location":"inference/#project-structure","text":"inference - pipeline / \u251c\u2500\u2500 app - data / # Database files \u251c\u2500\u2500 audio - data / # Input audio recordings \u251c\u2500\u2500 docker / # Docker configuration files \u251c\u2500\u2500 json - output / # Prediction results and reports \u251c\u2500\u2500 logs / # Application logs \u251c\u2500\u2500 monsoon_biodiversity_common / # Core library modules \u2502 \u251c\u2500\u2500 config . py # Model and system configuration \u2502 \u251c\u2500\u2500 dataset . py # Data loading and preprocessing \u2502 \u251c\u2500\u2500 db_model . py # Database models and schema \u2502 \u251c\u2500\u2500 model . py # Neural network architecture \u2502 \u2514\u2500\u2500 requirements . txt # Core dependencies \u251c\u2500\u2500 scripts / # Utility and deployment scripts \u251c\u2500\u2500 src / # Main application source code \u2502 \u251c\u2500\u2500 debug_audio_monitoring . py # Real-time audio monitoring \u2502 \u251c\u2500\u2500 debug_process_detections . py # Detection processing and reporting \u2502 \u251c\u2500\u2500 inference_station . py # Station-specific inference \u2502 \u251c\u2500\u2500 query . py # Database query utilities \u2502 \u2514\u2500\u2500 species_mapping . py # Species classification mapping \u251c\u2500\u2500 weights / # Pre-trained model weights \u2514\u2500\u2500 requirements . txt # Main project dependencies","title":"Project Structure"},{"location":"inference/#quick-start","text":"","title":"Quick Start"},{"location":"inference/#prerequisites","text":"Python 3.8+ Docker (optional, for containerized deployment) Audio processing libraries (librosa, torchaudio) Deep learning framework (PyTorch)","title":"Prerequisites"},{"location":"inference/#installation","text":"Clone the repository This one is already clone at iNET (server4 machine) bash git clone <repository-url> cd inference-pipeline git submodule update --init --recursive Download model weights The weights are already uploaded to the iNET and attached to the drive as well Place soundscape-model.pt file in the weights/ directory sound-scape.pt = sound classification model xgboost-model.pkl = score prediction model Ensure the model architecture matches the configuration in monsoon_biodiversity_common/config.py Setup database bash # The database will be automatically initialized on first run python src/debug_audio_monitoring.py","title":"Installation"},{"location":"inference/#docker-deployment","text":"Build production image bash ./scripts/build_prod_image.sh Run with docker-compose bash cd docker docker-compose -f docker-compose-devel.yaml up -d Access container bash docker exec -it dev-bio-diversity bash Run script bash sh run.sh it will run the audio-monitoring.py and process_detection.py and the intention of those python script will be explained below session","title":"Docker Deployment"},{"location":"inference/#configuration","text":"","title":"Configuration"},{"location":"inference/#database","text":"This is how the conceptual diagram works inside the inference data accepting","title":"Database"},{"location":"inference/#model-configuration","text":"Edit monsoon_biodiversity_common/config.py to customize: Audio parameters : Sample rate, mel bands, FFT settings Model architecture : Backbone model, number of classes Training settings : Learning rate, batch size, epochs","title":"Model Configuration"},{"location":"inference/#usage","text":"","title":"Usage"},{"location":"inference/#real-time-audio-monitoring","text":"Start the audio monitoring service: python src/audio_monitoring.py This service: - Monitors audio directories for new files - Processes audio files through the species detection model - Stores results in the database - Generates real-time logs","title":"Real-time Audio Monitoring"},{"location":"inference/#batch-processing","text":"Process existing audio files: python src/inference_station.py","title":"Batch Processing"},{"location":"inference/#query-results","text":"Query detection results from the database: python src/query.py","title":"Query Results"},{"location":"inference/#daily-reports","text":"Generate daily detection summaries: python src/debug_process_detections.py --schedule","title":"Daily Reports"},{"location":"inference/#database-schema","text":"","title":"Database Schema"},{"location":"inference/#core-tables","text":"RpiDevices : Device information and metadata AudioFiles : Audio file records and metadata SpeciesDetections : Detection results with confidence scores","title":"Core Tables"},{"location":"inference/#query-examples","text":"from src.query import query_species_by_device_and_dates # Query detections for specific device and dates results = query_species_by_device_and_dates ( engine , \"RPiID-0000000081519079\" , [ \"2025-04-23\" , \"2025-04-22\" ] )","title":"Query Examples"},{"location":"inference/#output-formats","text":"","title":"Output Formats"},{"location":"inference/#detection-results","text":"Species detections are stored with: - Audio file reference - Species classification - Confidence score - Temporal segment information - Device and timestamp metadata","title":"Detection Results"},{"location":"inference/#log-files","text":"audio_inference.log : Real-time processing logs batch_audio_inference.log : Batch processing logs daily_report.log : Daily summary reports","title":"Log Files"},{"location":"inference/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"inference/#common-issues","text":"Model weights not found Ensure soundscape-model.pt is in the weights/ directory Check file permissions and paths Audio directory not accessible Verify audio data directory exists and is readable Check Docker volume mounts if using containers Database connection errors Ensure SQLite database directory is writable Check database file permissions","title":"Common Issues"},{"location":"notes/","text":"This is the some important notes to take care of \u00b6","title":"Notes"},{"location":"notes/#this-is-the-some-important-notes-to-take-care-of","text":"","title":"This is the some important notes to take care of"},{"location":"score/","text":"This is the documentation for the score prediction model \u00b6","title":"Score prediction"},{"location":"score/#this-is-the-documentation-for-the-score-prediction-model","text":"","title":"This is the documentation for the score prediction model"},{"location":"training/","text":"This is the model training documentation \u00b6","title":"Model Training"},{"location":"training/#this-is-the-model-training-documentation","text":"","title":"This is the model training documentation"}]}