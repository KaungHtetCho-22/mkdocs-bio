{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Biodiversity Project Documentation","text":"<p>Welcome to the documentation hub for the Biodiversity Project.</p> <p>This site covers end-to-end guidance for deploying IoT devices, training AI models for eco-acoustic analysis, and running the production inference pipeline for biodiversity monitoring and scoring. It includes a minimal setup path to get you started quickly.</p> <p></p> <p>High-level architecture of data collection, AI inference, and reporting.</p>"},{"location":"#what-youll-find-here","title":"What you'll find here","text":"<ul> <li>Hardware setup and field deployment procedures</li> <li>AI model overview, datasets, and training approach</li> <li>Inference pipeline, database schema, and operations</li> </ul>"},{"location":"ai/","title":"Biodiversity score prediction","text":""},{"location":"ai/#overview","title":"Overview","text":"<p>This project predicts regional biodiversity scores through a two-stage workflow Figure 1: 1. bird and insect sound classification and the 2. biodiversity score level prediction.  </p> <ol> <li>Bird and Insect Sound Classification Audio recordings are collected by deployed AudioMoth devices. As illustrated in Figure 1, the recordings are preprocessed and fed into a deep learning classifier based on a modified implementation of the BirdCLEF 2023 4th Place Solution. The model identifies bird and insect species and also detects non-biological sounds such as human speech, other human-generated noises, and vehicle sounds. It was pre-trained on species recordings from Xeno-canto and noise recordings from our own data collection.</li> <li>Biodiversity Score Level Prediction For each region, the frequency of occurrence of every detected species and noise class is aggregated from the classified recordings. These frequencies serve as input features to a traditional machine learning model (XGBoost), which predicts the region\u2019s biodiversity score level: high, medium, or low.  </li> </ol> <p></p> <p>Figure 1: Biodiversity score level prediction overview.</p>"},{"location":"ai/#data","title":"Data","text":"<p>This section provides an overview of the data used in this project. We summarize the sources, label quality, use in training, geographic filtering, and the final class list.</p>"},{"location":"ai/#sources","title":"Sources","text":"<p>Public: Expert-labeled wildlife audio from Xeno-canto, covering birds and insects. Self-collected: Field recordings captured with AudioMoth devices in tea plantations around Chiang Mai, Thailand.</p>"},{"location":"ai/#label-quality","title":"Label quality","text":"<p>Xeno-canto recordings include expert-provided species labels. The self-collected recordings lack ground-truth annotations.</p>"},{"location":"ai/#use-in-training","title":"Use in training","text":"<p>The sound-classification model is trained primarily on the labeled Xeno-canto data. Additional noise examples from our self-collected recordings (e.g., human speech, human activity, vehicles, and other environmental noises) are included to improve robustness.</p>"},{"location":"ai/#geographic-filtering","title":"Geographic filtering","text":"<p>To reduce label noise and improve relevance, we removed species not known to occur in Thailand, with a particular focus on the Chiang Mai region.</p>"},{"location":"ai/#class-list","title":"Class list","text":"<p>The final set of bird and insect species used in training and inference is documented in species.txt.</p>"},{"location":"ai/#training-dataset","title":"Training dataset","text":"<p>The dataset consists of bird species, insect species, and noise classes.</p>"},{"location":"ai/#bird-species","title":"Bird species","text":"Common Name Biological Name Number of Samples Asian Koel Abroscopus-superciliaris 118 (Add more rows here)"},{"location":"ai/#insect-species","title":"Insect species","text":"Common Name Biological Name Number of Samples Cicada Cicadidae 800 (Add more rows here)"},{"location":"ai/#noise-classes","title":"Noise classes","text":"Class Name Description Number of Samples (Example) Rain Background rain noise 500 (Add more rows here)"},{"location":"ai/#model-architecture","title":"Model architecture","text":"<p>The sound classification system is based on:</p> <ul> <li>Input: Mel-spectrograms extracted from audio recordings.</li> <li>Feature Extraction: Convolutional Neural Networks (CNNs) for spatial feature learning.</li> <li>Classification Layer: Fully-connected layers with softmax output for multi-class classification.</li> <li>Training Details:</li> <li>Optimizer: Adam</li> <li>Loss: Categorical Cross-Entropy</li> <li>Learning Rate: (e.g., 0.001)</li> <li>Epochs: (e.g., 50)</li> <li>Batch Size: (e.g., 32)</li> </ul>"},{"location":"hardware/","title":"Hardware Components &amp; Setup Documentation","text":""},{"location":"hardware/#device-list","title":"Device list","text":"<p>This system consists of the following IoT hardware components:</p> <ol> <li>Raspberry Pi 3 B+ (64 GB microSD storage)  </li> <li>AudioMoth (Acoustic logger)  </li> <li>4G Router (with SIM card)  </li> <li>Internet SIM Card </li> <li>Solar Panel (with battery storage)  </li> </ol>"},{"location":"hardware/#raspberry-pi-setup","title":"Raspberry Pi setup","text":""},{"location":"hardware/#hardware","title":"Hardware","text":"<ul> <li>Model: Raspberry Pi 3 B+  </li> <li>Storage: 64 GB microSD card  </li> </ul>"},{"location":"hardware/#os-installation","title":"OS installation","text":"<ol> <li>Insert the SD card into your computer.  </li> <li>Use Balena Etcher (or similar) to flash the provided <code>.img</code> backup OS image.  </li> <li>Insert the flashed SD card into the Raspberry Pi.  </li> <li>Power on the device.  </li> </ol> Raspberry Pi SD Card <p>Download Raspberry Pi OS Image</p>"},{"location":"hardware/#login-credentials","title":"Login credentials","text":"<p>Default credentials (can be customized): - Username: <code>pi</code> - Password: <code>raspberry</code> </p>"},{"location":"hardware/#vpn-configuration","title":"VPN configuration","text":"<p>Each Raspberry Pi has its own OpenVPN account.</p>"},{"location":"hardware/#file-structure","title":"File structure","text":"<p>Navigate to the OpenVPN directory:</p> <pre><code>cd /etc/openvpn/\nls\n</code></pre> <p>Expected files: <pre><code>client/\ncredentials.txt\nopenvpn_MONSOON_TEA05.conf\nserver/\nupdate-resolv-conf\n</code></pre></p> <ul> <li><code>openvpn_MONSOON_TEA05.conf</code> \u2192 Converted <code>.ovpn</code> client config file  </li> <li><code>credentials.txt</code> \u2192 VPN username &amp; password (two lines only)</li> </ul>"},{"location":"hardware/#credentials-setup","title":"Credentials setup","text":"<p>Check credentials: <pre><code>cat /etc/openvpn/credentials.txt\n</code></pre> Format: <pre><code>vpn_username\nvpn_password\n</code></pre> Secure file permissions: <pre><code>sudo chmod 600 /etc/openvpn/credentials.txt\n</code></pre></p>"},{"location":"hardware/#vpn-service-setup","title":"VPN service setup","text":"<p>Enable &amp; start service: <pre><code>sudo systemctl enable openvpn@openvpn_MONSOON_TEA05\nsudo systemctl start openvpn@openvpn_MONSOON_TEA05\n</code></pre></p> <p>Manual connect: <pre><code>sudo openvpn --config openvpn_MONSOON_TEA05.ovpn\n</code></pre></p> <p>Verify connection: <pre><code>ifconfig\n</code></pre> VPN tunnel should point to <code>10.81.234.5</code>.</p>"},{"location":"hardware/#device-configuration-file","title":"Device configuration file","text":"<p>Example <code>config.json</code>: <pre><code>{\n    \"ftp\": {\n        \"uname\": \"monsoon\",\n        \"pword\": \"p8z3%1P#04\",\n        \"host\": \"192.168.70.5/production-workflow-ec2\",\n        \"use_ftps\": 1\n    },\n    \"offline_mode\": 0,\n    \"sensor\": {\n        \"sensor_index\": 2,\n        \"sensor_type\": \"USBSoundcardMic\",\n        \"record_length\": 600,\n        \"compress_data\": false,\n        \"capture_delay\": 0\n    },\n    \"sys\": {\n        \"working_dir\": \"/home/pi/tmp_dir\",\n        \"upload_dir\": \"/home/pi/continuous_monitoring_data\",\n        \"reboot_time\": \"02:00\"\n    },\n    \"device_id\": \"00000000f1c084c2\"\n}\n</code></pre></p>"},{"location":"hardware/#automatic-recording-service","title":"Automatic recording service","text":"<p>Example systemd service (<code>/etc/systemd/system/shellscript.service</code>): <pre><code>[Unit]\nDescription=My Shell Script\n\n[Service]\nExecStart=/home/pi/custom-pi-setup/recorder_startup_script.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Check live service logs: <pre><code>journalctl -u shellscript.service -f\n</code></pre></p>"},{"location":"hardware/#important-commands","title":"Important commands","text":"Command Purpose <code>arecord -l</code> List available recording devices <code>journalctl -u shellscript.service -f</code> Live monitoring of recording service <code>sudo systemctl restart shellscript.service</code> Restart recording service"},{"location":"hardware/#audiomoth-setup","title":"AudioMoth setup","text":""},{"location":"hardware/#overview","title":"Overview","text":"<p>AudioMoth is a low-cost, full-spectrum acoustic logger, based on the Gecko processor range from Silicon Labs. It can record audible and ultrasonic frequencies at rates from 8,000 to 384,000 samples/sec. It is used in two modes: mobile and station.</p>"},{"location":"hardware/#modes","title":"Modes","text":""},{"location":"hardware/#mobile-type","title":"Mobile Type","text":"<ul> <li>Portable configuration for temporary deployments  </li> <li>Ideal for short-term surveys Download Mobile AudioMoth Manual (PDF) </li> </ul>"},{"location":"hardware/#station-type","title":"Station Type","text":"<ul> <li>Fixed position setup for continuous monitoring  </li> <li>Powered by solar &amp; external battery Download IoT Station Setup Manual (PDF) </li> </ul>"},{"location":"hardware/#router-setup-troubleshooting","title":"Router setup &amp; troubleshooting","text":"<ul> <li>Type: 4G Router with SIM  </li> <li>Purpose: Internet connection for remote locations  </li> </ul> <p>Troubleshooting Checklist: 1. Check LED status indicators 2. Ensure SIM card is active 3. Restart router if connection drops  </p>"},{"location":"hardware/#solar-panel-battery","title":"Solar panel &amp; battery","text":""},{"location":"hardware/#solar-panel","title":"Solar panel","text":"<ul> <li>Powers IoT devices in remote areas  </li> <li>Indicators: </li> <li>Green \u2192 Charging  </li> <li>Red \u2192 Low battery  </li> <li>Off \u2192 No power  </li> </ul>"},{"location":"hardware/#battery","title":"Battery","text":"<ul> <li>Stores energy for night/cloudy use  </li> <li>Blink Indicators: </li> <li>1 blink \u2192 Low  </li> <li>2 blinks \u2192 Medium  </li> <li>3 blinks \u2192 Full  </li> </ul>"},{"location":"hardware/#system-workflow","title":"System workflow","text":"<ol> <li>Power Supply \u2192 Solar Panel \u2192 Battery \u2192 Raspberry Pi &amp; Router  </li> <li>Data Capture \u2192 AudioMoth or Raspberry Pi records audio  </li> <li>Data Transmission \u2192 Router sends via 4G  </li> <li>Remote Access \u2192 VPN connection for management  </li> <li>Monitoring \u2192 Logs checked via <code>journalctl</code> or SSH  </li> </ol>"},{"location":"inference/","title":"Bird Sound Monitoring &amp; Scoring Pipeline","text":""},{"location":"inference/#overview","title":"Overview","text":"<p>This system automates the end-to-end process of monitoring bird sounds using IoT devices, classifying them with a soundscape model, predicting biodiversity scores, and delivering the results as JSON payloads to an API.</p> <p></p> <p>Figure 1: Bird Sound Monitoring &amp; Scoring Pipeline Overview</p>"},{"location":"inference/#data-source-raspberry-pi","title":"Data source \u2013 Raspberry Pi","text":"<ul> <li>Device: Raspberry Pi with Audiomoth sensors for continuous field audio collection.  </li> <li>Protocol: FTPS (Secure FTP) for encrypted data transfer.  </li> <li>Destination: Audio files are securely uploaded to the iNet private cloud.</li> </ul>"},{"location":"inference/#audio-collection","title":"Audio collection","text":"<ul> <li>Captures 10-minute audio clips in <code>.WAV</code> format.</li> </ul>"},{"location":"inference/#bird-classification-model","title":"Bird classification model","text":"<ul> <li>Processes audio clips using a deep learning soundscape model.  </li> <li>Identifies bird species with confidence scores.  </li> <li>Stores classification results in a MySQL database for further analysis.</li> </ul>"},{"location":"inference/#score-prediction-model","title":"Score prediction model","text":"<ul> <li>Retrieves classification results from MySQL.  </li> <li>Generates a biodiversity score for the monitored area based on detected species:  </li> <li>Score A \u2013 High biodiversity  </li> <li>Score B \u2013 Medium biodiversity  </li> <li>Score C \u2013 Low biodiversity  </li> <li>Outputs results to an API in structured JSON format.</li> </ul> Component Description Raspberry Pi + Audiomoth + 4G Router Collects audio data from the field. FTPS Secure file transfer protocol for uploading audio files. iNet Server Runs inference processes for species detection. Bird Classification Model Analyzes audio clips and identifies bird species with confidence scores. MySQL Stores classification results and associated metadata. Score Prediction Model Generates biodiversity scores based on classification results. API Receives JSON payloads containing final scoring results."},{"location":"inference/#pipeline-file-structure","title":"Pipeline file structure","text":"<pre><code>inference-pipeline/\n\u251c\u2500\u2500 app-data/                    # Database files\n\u251c\u2500\u2500 audio-data/                  # Input audio recordings\n\u251c\u2500\u2500 docker/                      # Docker configuration files\n\u251c\u2500\u2500 json-output/                 # Prediction results and reports\n\u251c\u2500\u2500 logs/                        # Application logs\n\u251c\u2500\u2500 monsoon_biodiversity_common/ # Core library modules\n\u2502   \u251c\u2500\u2500 config.py                # Model and system configuration\n\u2502   \u251c\u2500\u2500 dataset.py               # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 db_model.py              # Database models and schema\n\u2502   \u251c\u2500\u2500 model.py                 # Neural network architecture\n\u251c\u2500\u2500 scripts/                     # Utility and deployment scripts\n\u251c\u2500\u2500 src/                         # Main application source code\n\u2502   \u251c\u2500\u2500 audio_monitoring.py      # Real-time audio monitoring\n\u2502   \u251c\u2500\u2500 process_detections.py    # Detection processing and reporting\n\u2502   \u2514\u2500\u2500 species_mapping.py       # Species classification mapping (Thai - Eng)\n\u251c\u2500\u2500 weights/                     # Pre-trained model weights\n\u2514\u2500\u2500 requirements.txt             # Main project libraries\n</code></pre>"},{"location":"inference/#components","title":"Components","text":""},{"location":"inference/#main-functions","title":"Main functions","text":"<ul> <li>File Monitoring: Watchdog-based file system monitoring for automatic processing</li> <li>Audio Processing: Real-time audio file monitoring and processing</li> <li>Deep Learning Models: Attention-based neural network for species classification and xgboost model for prediction the score of the area.</li> <li>Database: SQLite database for storing detections results</li> <li>Docker Support: Containerized deployment for development and production</li> </ul> <p>NOTES: This deployment is already setup on the iNET and now it is working in actions. This documentation guides for making the deployment anywhere else.</p>"},{"location":"inference/#file-monitoring","title":"File monitoring","text":""},{"location":"inference/#audio-processing","title":"Audio processing","text":""},{"location":"inference/#deep-learning-models","title":"Deep learning models","text":"<p>This AI models can be read from AI Models.</p>"},{"location":"inference/#database-schema","title":"Database schema","text":"<p>This is how the conceptual diagram works inside the inference data accepting</p> Table Name Description RpiDevices Stores device information and associated metadata. AudioFiles Contains audio file records along with relevant metadata. SpeciesDetections Holds species detection results with confidence scores and related attributes. <p></p>"},{"location":"inference/#docker-support","title":"Docker support","text":""},{"location":"inference/#quick-start","title":"Quick start","text":""},{"location":"inference/#model-configuration","title":"Model configuration","text":"<p>Edit <code>monsoon_biodiversity_common/config.py</code> to customize:</p> <ul> <li>Audio parameters: Sample rate, mel bands, FFT settings</li> <li>Model architecture: Backbone model, number of classes</li> <li>Training settings: Learning rate, batch size, epochs</li> </ul>"},{"location":"inference/#example-usage","title":"Example usage","text":""},{"location":"inference/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>Audio processing libraries (librosa, torchaudio)</li> <li>Deep learning framework (PyTorch)</li> </ul>"},{"location":"inference/#installation","title":"Installation","text":"<ol> <li>Clone the repository This one is already clone at iNET (server4 machine)</li> </ol> <pre><code>git clone &lt;repository-url&gt;\ncd inference-pipeline\ngit submodule update --init --recursive \n</code></pre> <ol> <li> <p>Download model weights The weights are already uploaded to the iNET and attached to the drive as well</p> </li> <li> <p>Place <code>soundscape-model.pt</code> file in the <code>weights/</code> directory</p> </li> <li>sound-scape.pt = sound classification model </li> <li>xgboost-model.pkl = score prediction model </li> <li> <p>Ensure the model architecture matches the configuration in <code>monsoon_biodiversity_common/config.py</code></p> </li> <li> <p>Setup database <pre><code># The database will be automatically initialized on first run\npython src/debug_audio_monitoring.py\n</code></pre></p> </li> <li> <p>Build production image <pre><code>sh scripts/build_prod_image.sh\n</code></pre></p> </li> <li> <p>Run with docker-compose <pre><code>docker compose -f docker/docker-compose-prod.yaml up -d\n</code></pre></p> </li> <li> <p>Access container <pre><code>docker exec -it prod-bio-service bash\n</code></pre></p> </li> <li> <p>Check logs <pre><code>docker logs -f prod-bio-service \n</code></pre></p> </li> </ol>"},{"location":"inference/#real-time-audio-monitoring","title":"Real-time Audio Monitoring","text":"<p>Start the audio monitoring service:</p> <pre><code>python src/audio_monitoring.py\n</code></pre> <p>This service: - Monitors audio directories for new files - Processes audio files through the species detection model - Stores results in the database - Generates real-time logs</p>"}]}