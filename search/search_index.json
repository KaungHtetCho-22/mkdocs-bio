{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Biodiversity Project Documentation","text":"<p>Welcome to the documentation hub for the Biodiversity Project.</p> <p>This covers end-to-end guidance for deploying IoT devices, training AI models for eco-acoustic analysis, and running the production inference pipeline for biodiversity monitoring and scoring. It includes a minimal setup path to get started quickly.</p> <p></p> <p>High-level architecture of data collection, AI inference, and reporting.</p>"},{"location":"#what-youll-find-here","title":"What you'll find here","text":"<ul> <li>Hardware setup and field deployment procedures</li> <li>AI model overview, datasets, and training approach</li> <li>Inference pipeline, database schema, and operations</li> </ul>"},{"location":"ai/","title":"Biodiversity score prediction","text":""},{"location":"ai/#overview","title":"Overview","text":"<p>This project predicts regional biodiversity scores through a two-stage workflow Figure 1: 1. bird and insect sound classification and the 2. biodiversity score level prediction.  </p> <ol> <li>Bird and Insect Sound Classification Audio recordings are collected by deployed AudioMoth devices. As illustrated in Figure 1, the recordings are preprocessed and fed into a deep learning classifier based on a modified implementation of the BirdCLEF 2023 4th Place Solution. The model identifies bird and insect species and also detects non-biological sounds such as human speech, other human-generated noises, and vehicle sounds. It was pre-trained on species recordings from Xeno-canto and noise recordings from our own data collection.</li> <li>Biodiversity Score Level Prediction For each region, the frequency of occurrence of every detected species and noise class is aggregated from the classified recordings. These frequencies serve as input features to a traditional machine learning model (XGBoost), which predicts the region\u2019s biodiversity score level: high, medium, or low.  </li> </ol> <p></p> <p>Figure 1: Biodiversity score level prediction overview.</p>"},{"location":"ai/#data","title":"Data","text":"<p>This section provides an overview of the data used in this project. We summarize the sources, label quality, use in training, geographic filtering, and the final class list.</p>"},{"location":"ai/#sources","title":"Sources","text":"<p>Public: Expert-labeled wildlife audio from Xeno-canto, covering birds and insects. Self-collected: Field recordings captured with AudioMoth devices in tea plantations around Chiang Mai, Thailand.</p>"},{"location":"ai/#label-quality","title":"Label quality","text":"<p>Xeno-canto recordings include expert-provided species labels. The self-collected recordings lack ground-truth annotations.</p>"},{"location":"ai/#use-in-training","title":"Use in training","text":"<p>The sound-classification model is trained primarily on the labeled Xeno-canto data. Additional noise examples from our self-collected recordings (e.g., human speech, human activity, vehicles, and other environmental noises) are included to improve robustness.</p>"},{"location":"ai/#geographic-filtering","title":"Geographic filtering","text":"<p>To reduce label noise and improve relevance, we removed species not known to occur in Thailand, with a particular focus on the Chiang Mai region.</p>"},{"location":"ai/#class-list","title":"Class list","text":"<p>The final set of bird and insect species used in training and inference is documented in species.txt.</p>"},{"location":"ai/#training-dataset","title":"Training dataset","text":"<p>The dataset consists of bird species, insect species, and noise classes.</p>"},{"location":"ai/#bird-species","title":"Bird species","text":"Common Name Biological Name Number of Samples Asian Koel Abroscopus-superciliaris 118 (Add more rows here)"},{"location":"ai/#insect-species","title":"Insect species","text":"Common Name Biological Name Number of Samples Cicada Cicadidae 800 (Add more rows here)"},{"location":"ai/#noise-classes","title":"Noise classes","text":"Class Name Description Number of Samples (Example) Rain Background rain noise 500 (Add more rows here)"},{"location":"ai/#model-architecture","title":"Model architecture","text":"<p>The sound classification system is based on:</p> <ul> <li>Input: Mel-spectrograms extracted from audio recordings.</li> <li>Feature Extraction: Convolutional Neural Networks (CNNs) for spatial feature learning.</li> <li>Classification Layer: Fully-connected layers with softmax output for multi-class classification.</li> <li>Training Details:</li> <li>Optimizer: Adam</li> <li>Loss: Categorical Cross-Entropy</li> <li>Learning Rate: (e.g., 0.001)</li> <li>Epochs: (e.g., 50)</li> <li>Batch Size: (e.g., 32)</li> </ul>"},{"location":"hardware/","title":"Hardware Components &amp; Setup Documentation","text":""},{"location":"hardware/#device-list","title":"Device list","text":"<p>This system consists of the following IoT hardware components:</p> <ol> <li>Raspberry Pi 3 B+ (64 GB microSD storage)  </li> <li>AudioMoth (Acoustic logger)  </li> <li>4G Router (with SIM card)  </li> <li>Internet SIM Card </li> <li>Solar Panel (with battery storage)  </li> </ol>"},{"location":"hardware/#raspberry-pi-setup","title":"Raspberry Pi setup","text":""},{"location":"hardware/#hardware","title":"Hardware","text":"<ul> <li>Model: Raspberry Pi 3 B+  </li> <li>Storage: 64 GB microSD card  </li> </ul>"},{"location":"hardware/#os-installation","title":"OS installation","text":"<ol> <li>Insert the SD card into your computer.  </li> <li>Use Balena Etcher (or similar) to flash the provided <code>.img</code> backup OS image.  </li> <li>Insert the flashed SD card into the Raspberry Pi.  </li> <li>Power on the device.  </li> </ol> <p>Download Raspberry Pi OS Image</p>"},{"location":"hardware/#login-credentials","title":"Login credentials","text":"<p>Default credentials (can be customized): - Username: <code>pi</code> - Password: <code>raspberry</code> </p>"},{"location":"hardware/#vpn-configuration","title":"VPN configuration","text":"<p>Each Raspberry Pi has its own OpenVPN account.</p>"},{"location":"hardware/#file-structure","title":"File structure","text":"<p>Navigate to the OpenVPN directory:</p> <pre><code>cd /etc/openvpn/\nls\n</code></pre> <p>Expected files: <pre><code>client/\ncredentials.txt\nopenvpn_MONSOON_TEA05.conf\nserver/\nupdate-resolv-conf\n</code></pre></p> <ul> <li><code>openvpn_MONSOON_TEA05.conf</code> \u2192 Converted <code>.ovpn</code> client config file  </li> <li><code>credentials.txt</code> \u2192 VPN username &amp; password (two lines only)</li> </ul>"},{"location":"hardware/#credentials-setup","title":"Credentials setup","text":"<p>Check credentials: <pre><code>cat /etc/openvpn/credentials.txt\n</code></pre> Format: <pre><code>vpn_username\nvpn_password\n</code></pre> Secure file permissions: <pre><code>sudo chmod 600 /etc/openvpn/credentials.txt\n</code></pre></p>"},{"location":"hardware/#vpn-service-setup","title":"VPN service setup","text":"<p>Enable &amp; start service: <pre><code>sudo systemctl enable openvpn@openvpn_MONSOON_TEA05\nsudo systemctl start openvpn@openvpn_MONSOON_TEA05\n</code></pre></p> <p>Manual connect: <pre><code>sudo openvpn --config openvpn_MONSOON_TEA05.ovpn\n</code></pre></p> <p>Verify connection: <pre><code>ifconfig\n</code></pre> VPN tunnel should point to <code>10.81.234.5</code>.</p>"},{"location":"hardware/#device-configuration-file","title":"Device configuration file","text":"<p>Example <code>config.json</code>: <pre><code>{\n    \"ftp\": {\n        \"uname\": \"monsoon\",\n        \"pword\": \"p8z3%1P#04\",\n        \"host\": \"192.168.70.5/production-workflow-ec2\",\n        \"use_ftps\": 1\n    },\n    \"offline_mode\": 0,\n    \"sensor\": {\n        \"sensor_index\": 2,\n        \"sensor_type\": \"USBSoundcardMic\",\n        \"record_length\": 600,\n        \"compress_data\": false,\n        \"capture_delay\": 0\n    },\n    \"sys\": {\n        \"working_dir\": \"/home/pi/tmp_dir\",\n        \"upload_dir\": \"/home/pi/continuous_monitoring_data\",\n        \"reboot_time\": \"02:00\"\n    },\n    \"device_id\": \"00000000f1c084c2\"\n}\n</code></pre></p>"},{"location":"hardware/#automatic-recording-service","title":"Automatic recording service","text":"<p>Example systemd service (<code>/etc/systemd/system/shellscript.service</code>): <pre><code>[Unit]\nDescription=My Shell Script\n\n[Service]\nExecStart=/home/pi/custom-pi-setup/recorder_startup_script.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Check live service logs: <pre><code>journalctl -u shellscript.service -f\n</code></pre></p>"},{"location":"hardware/#important-commands","title":"Important commands","text":"Command Purpose <code>arecord -l</code> List available recording devices <code>journalctl -u shellscript.service -f</code> Live monitoring of recording service <code>sudo systemctl restart shellscript.service</code> Restart recording service"},{"location":"hardware/#audiomoth-setup","title":"AudioMoth setup","text":""},{"location":"hardware/#overview","title":"Overview","text":"<p>AudioMoth is a low-cost, full-spectrum acoustic logger, based on the Gecko processor range from Silicon Labs. It can record audible and ultrasonic frequencies at rates from 8,000 to 384,000 samples/sec. It is used in two modes: mobile and station.</p>"},{"location":"hardware/#modes","title":"Modes","text":""},{"location":"hardware/#mobile-type","title":"Mobile Type","text":"<ul> <li>Portable configuration for temporary deployments  </li> <li>Ideal for short-term surveys Download Mobile AudioMoth Manual (PDF) </li> </ul>"},{"location":"hardware/#station-type","title":"Station Type","text":"<ul> <li>Fixed position setup for continuous monitoring  </li> <li>Powered by solar &amp; external battery Download IoT Station Setup Manual (PDF) </li> </ul>"},{"location":"hardware/#router-setup-troubleshooting","title":"Router setup &amp; troubleshooting","text":"<ul> <li>Type: 4G Router with SIM  </li> <li>Purpose: Internet connection for remote locations  </li> </ul> <p>Troubleshooting Checklist: 1. Check LED status indicators 2. Ensure SIM card is active 3. Restart router if connection drops  </p>"},{"location":"hardware/#solar-panel-battery","title":"Solar panel &amp; battery","text":""},{"location":"hardware/#solar-panel","title":"Solar panel","text":"<ul> <li>Powers IoT devices in remote areas  </li> <li>Indicators: </li> <li>Green \u2192 Charging  </li> <li>Red \u2192 Low battery  </li> <li>Off \u2192 No power  </li> </ul>"},{"location":"hardware/#battery","title":"Battery","text":"<ul> <li>Stores energy for night/cloudy use  </li> <li>Blink Indicators: </li> <li>1 blink \u2192 Low  </li> <li>2 blinks \u2192 Medium  </li> <li>3 blinks \u2192 Full  </li> </ul>"},{"location":"hardware/#system-workflow","title":"System workflow","text":"<ol> <li>Power Supply \u2192 Solar Panel \u2192 Battery \u2192 Raspberry Pi &amp; Router  </li> <li>Data Capture \u2192 AudioMoth or Raspberry Pi records audio  </li> <li>Data Transmission \u2192 Router sends via 4G  </li> <li>Remote Access \u2192 VPN connection for management  </li> <li>Monitoring \u2192 Logs checked via <code>journalctl</code> or SSH  </li> </ol>"},{"location":"inference/","title":"Bird Sound Monitoring &amp; Scoring Pipeline","text":""},{"location":"inference/#overview","title":"Overview","text":"<p>This system automates the end-to-end process of monitoring bird sounds using IoT devices, classifying them with a soundscape model, predicting biodiversity scores, and delivering the results as JSON payloads to an API.</p> <p>Repository: inference-workflow\u2011iNet</p> <p></p> <p>Figure 1: Bird Sound Monitoring &amp; Scoring Pipeline Overview</p>"},{"location":"inference/#data-source-raspberry-pi","title":"Data source \u2013 Raspberry Pi","text":"<ul> <li>Device: Raspberry Pi with Audiomoth sensors for continuous field audio collection.  </li> <li> <p>Protocol: FTPS (Secure FTP) for encrypted data transfer.  </p> </li> <li> <p>iNet Server 4 Machine Information:</p> </li> </ul> Parameter Value Username monsoon IP Address 192.168.70.5 Password p8z3%1P#04 <ul> <li>Destination: Audio files are securely uploaded to the iNet private cloud.</li> </ul>"},{"location":"inference/#audio-collection","title":"Audio collection","text":"<ul> <li>Captures 10-minute audio clips in <code>.WAV</code> format.</li> <li>Sample audio files can be downloaded from this Google Drive link</li> <li>See attached how to fetch audio from filezilla setup manual Filezilla setup manual</li> </ul>"},{"location":"inference/#bird-classification-model","title":"Bird classification model","text":"<ul> <li>Processes audio clips using a deep learning soundscape model.  </li> <li>Identifies bird species with confidence scores.  </li> <li>Stores classification results in a SQLite database for further analysis.</li> </ul>"},{"location":"inference/#score-prediction-model","title":"Score prediction model","text":"<ul> <li>Retrieves classification results from SQLite.  </li> <li>Generates a biodiversity score for the monitored area based on detected species:  </li> <li>Score A \u2013 High biodiversity  </li> <li>Score B \u2013 Medium biodiversity  </li> <li>Score C \u2013 Low biodiversity  </li> <li>Outputs results to an API in structured JSON format.</li> </ul> Component Description Raspberry Pi + Audiomoth + 4G Router Collects audio data from the field FTPS Secure file transfer protocol for uploading audio files iNet Server Runs inference processes for species detection Bird Classification Model Analyzes audio clips and identifies bird species + confidence SQLite Stores classification results and associated metadata Score Prediction Model Generates biodiversity scores from classification results API Receives JSON payloads containing final scoring results"},{"location":"inference/#pipeline-file-structure","title":"Pipeline file structure","text":"<pre><code>inference-pipeline/\n\u251c\u2500\u2500 app-data/                    # Database files\n\u251c\u2500\u2500 audio-data/                  # Input audio recordings\n\u251c\u2500\u2500 docker/                      # Docker configuration files\n\u251c\u2500\u2500 json-output/                 # Prediction results and reports\n\u251c\u2500\u2500 logs/                        # Application logs\n\u251c\u2500\u2500 monsoon_biodiversity_common/ # Core library modules\n\u2502   \u251c\u2500\u2500 config.py                # Model and system configuration\n\u2502   \u251c\u2500\u2500 dataset.py               # Data loading and preprocessing\n\u2502   \u251c\u2500\u2500 db_model.py              # Database models and schema\n\u2502   \u251c\u2500\u2500 model.py                 # Neural network architecture\n\u251c\u2500\u2500 scripts/                     # Utility and deployment scripts\n\u251c\u2500\u2500 src/                         # Main application source code\n\u2502   \u251c\u2500\u2500 audio_monitoring.py      # Real-time audio monitoring\n\u2502   \u251c\u2500\u2500 process_detections.py    # Detection processing and reporting\n\u2502   \u2514\u2500\u2500 species_mapping.py       # Species classification mapping (Thai - Eng)\n\u251c\u2500\u2500 weights/                     # Pre-trained model weights\n\u2514\u2500\u2500 requirements.txt             # Main project libraries\n</code></pre>"},{"location":"inference/#components","title":"Components","text":""},{"location":"inference/#main-functions","title":"Main functions","text":"<ul> <li>File Monitoring: Watchdog-based file system monitoring for automatic processing</li> <li>Audio Processing: Real-time audio file monitoring and processing</li> <li>Deep Learning Models: Attention-based neural network for species classification and xgboost model for prediction the score of the area.</li> <li>Database: SQLite database for storing detections results</li> <li>Docker Support: Containerized deployment for development and production</li> </ul> <p>Note: This deployment is already set up on iNet and running in production. Use this guide to deploy elsewhere.</p>"},{"location":"inference/#file-monitoring","title":"File Monitoring","text":"<p>Location: <code>src/audio_monitoring.py</code></p> <p>Purpose: Continuously watches incoming audio, validates file stability, performs 5\u2011second window classification, and writes per\u2011segment detections to the database with resilient logging.</p> <p>Watched paths and filters: - Root directory: <code>/app/audio-data/</code> - Subfolders monitored: Only folders whose names contain <code>RPiID</code> (e.g., <code>RPiID-001</code>), monitored recursively - File types: <code>.wav</code>, <code>.ogg</code>, <code>.mp3</code></p> <p>Monitoring and queueing: - Uses <code>watchdog</code> (<code>Observer</code> + <code>FileSystemEventHandler</code>) to enqueue new files into a thread-safe <code>queue.Queue</code> - A dedicated background thread consumes the queue and processes files sequentially</p> <p>File stability gate: - Before inference, the service waits for the file size to remain unchanged and non\u2011zero for 3 consecutive seconds - Maximum wait window: 60 seconds; unstable or empty files are skipped with an error log</p> <p>Inference details: - Loads <code>AttModel</code> with weights at <code>/app/weights/soundscape-model.pt</code> - Audio loaded with <code>librosa</code> at 32 kHz; split into 5\u2011second segments: seconds = 5, 10, 15, ... up to clip length - For each segment, computes sigmoid probabilities for <code>CFG.target_columns</code> and selects:   - <code>Class</code>: argmax label   - <code>Score</code>: max probability - Segment identifier <code>row_id</code> format: <code>&lt;file_stem&gt;_&lt;second&gt;</code></p> <p>Database write (SQLite): - DB URL: <code>sqlite:////app/app-data/soundscape-model.db</code> - Entities used: <code>RpiDevices</code>, <code>AudioFiles</code>, <code>SpeciesDetections</code> - Path parsing: expects folder layout <code>.../RPiID-XXX/YYYY-MM-DD/&lt;audio_file&gt;</code> - Unique <code>AudioFiles.file_key</code>: <code>&lt;pi_id&gt;_&lt;recording_date&gt;_&lt;audio_filename&gt;</code> - For each segment, inserts a <code>SpeciesDetections</code> row with:   - <code>time_segment_id</code> = <code>row_id</code>   - <code>species_class</code> = predicted <code>Class</code>   - <code>confidence_score</code> = <code>Score</code>   - <code>created_at</code> = current UTC timestamp</p> <p>Logging: - Console: Colored via <code>colorlog</code> - File: <code>/app/logs/audio_inference.log</code></p> <p>File retention: - Files are currently kept after processing; a safe delete helper exists but is disabled by default</p> <p>Summary: Real\u2011time, resilient ingestion that only processes fully\u2011written files, performs 5\u2011second window classification, and persists detections per segment.</p>"},{"location":"inference/#audio-processing-detection-model","title":"Audio processing &amp; detection model","text":"<p>Locations: <code>src/process_detections.py</code>, <code>src/species_mapping.py</code></p> <p>Daily aggregation and scoring: - Loads an XGBoost model from <code>/app/weights/xgboost-model.pkl</code> - Reads detections from SQLite (<code>sqlite:///app-data/soundscape-model.db</code>) for a target date (default: yesterday) - Builds a feature table per device and hour by counting detections per species - Ensures a fixed feature set (<code>SELECTED_FEATURES</code>), filling missing species with zeros - Predicts per\u2011row scores and assigns the device score by majority vote across its hourly rows</p> <p>Hourly bucketing logic: - <code>AudioFiles.file_key</code> encodes a start time: <code>..._&lt;YYYY-MM-DD&gt;_&lt;HH&gt;-&lt;MM&gt;-&lt;SS&gt;</code> - For each <code>SpeciesDetections.time_segment_id</code> (<code>..._&lt;relative_second&gt;</code>), computes <code>absolute_second = start_time_in_seconds + relative_second</code> - Buckets into <code>hour = clamp(floor(absolute_second / 3600), 0..23)</code>; fallback <code>hour=0</code> if parsing fails</p> <p>Species categorization and localization: - Species are tagged as <code>bird</code> or <code>insect</code> via sets in <code>process_detections.py</code> - English/Thai display names are pulled from <code>SPECIES_INFO</code> in <code>species_mapping.py</code></p> <p>JSON output (saved and sent): - Saves to <code>json-output/predictions_&lt;YYYY-MM-DD&gt;.json</code> - Payload per device: <pre><code>{\n  \"&lt;DEVICE_ID&gt;\": [\n    {\n      \"date\": \"YYYYMMDD\",\n      \"coordinate\": [18.8018, 98.9948],\n      \"score\": 5,\n      \"species\": [\n        {\n          \"name_en\": \"Blue-throated Barbet\",\n          \"name_th\": \"\u0e19\u0e01\u0e42\u0e1e\u0e23\u0e30\u0e14\u0e01\u0e04\u0e32\u0e07\u0e1f\u0e49\u0e32\",\n          \"type\": \"bird\",\n          \"data\": [\"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"]\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>API delivery and retries: - Obtains OAuth token using env vars: <code>TOKEN_URL</code>, <code>CLIENT_ID</code>, <code>CLIENT_SECRET</code>, <code>API_USERNAME</code>, <code>API_PASSWORD</code> - Sends JSON to <code>API_URL</code> with <code>Authorization: Bearer &lt;token&gt;</code> - Up to 10 attempts with exponential backoff; skips sending if token cannot be obtained - Logs to <code>/app/logs/daily_report.log</code></p> <p>CLI usage: - <code>python process_detections.py --date YYYY-MM-DD</code> \u2014 run for a specific date - <code>python process_detections.py --now</code> \u2014 run immediately for today - <code>python process_detections.py --schedule</code> \u2014 run daily at 23:59</p>"},{"location":"inference/#deep-learning-models","title":"Deep learning models","text":"<p>This AI models can be read from AI Models.</p>"},{"location":"inference/#database-schema","title":"Database schema","text":"<p>The database schema is defined using SQLAlchemy ORM. The schema consists of three main tables, each with relationships and constraints as described below:</p> <p>1. RpiDevices - Purpose: Stores information about each Raspberry Pi device. - Key Columns:   - <code>id</code> (Primary Key)   - <code>pi_id</code> (Unique string identifier for the device)   - <code>pi_type</code> (Integer: 0 = Mobile, 1 = Station; default is 1) - Relationships: One-to-many with <code>AudioFiles</code> (a device can have multiple audio files).</p> <p>2. AudioFiles - Purpose: Records metadata for each audio file collected. - Key Columns:   - <code>id</code> (Primary Key)   - <code>device_id</code> (Foreign Key to <code>RpiDevices.id</code>)   - <code>recording_date</code> (Date of recording)   - <code>file_key</code> (Unique file identifier) - Constraints: Unique constraint on (<code>device_id</code>, <code>recording_date</code>, <code>file_key</code>) to prevent duplicate entries per device and date. - Relationships: One-to-many with <code>SpeciesDetections</code> (an audio file can have multiple detections).</p> <p>3. SpeciesDetections - Purpose: Stores the results of species detection for each audio file segment. - Key Columns:   - <code>id</code> (Primary Key)   - <code>audio_file_id</code> (Foreign Key to <code>AudioFiles.id</code>)   - <code>species_class</code> (Detected species name)   - <code>confidence_score</code> (Detection confidence, float)   - <code>created_at</code> (Timestamp of detection)   - <code>time_segment_id</code> (String identifier for the segment within the audio file) - Relationships: Many-to-one with <code>AudioFiles</code>.</p> <p>Entity Relationship Overview: - Each <code>RpiDevices</code> entry can have multiple <code>AudioFiles</code>. - Each <code>AudioFiles</code> entry can have multiple <code>SpeciesDetections</code>.</p> <p></p>"},{"location":"inference/#docker-support","title":"Docker support","text":"<ol> <li> <p>Build production image <pre><code>sh scripts/build_prod_image.sh\n</code></pre></p> </li> <li> <p>Run with docker-compose <pre><code>docker compose -f docker/docker-compose-prod.yaml up -d\n</code></pre></p> </li> <li> <p>Access container <pre><code>docker exec -it prod-bio-service bash\n</code></pre></p> </li> <li> <p>Check logs <pre><code>docker logs -f prod-bio-service \n</code></pre></p> </li> </ol>"},{"location":"inference/#example-usage","title":"Example usage","text":""},{"location":"inference/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository (with submodules) <pre><code>git clone &lt;repository-url&gt;\ncd inference-pipeline\ngit submodule update --init --recursive\n</code></pre></p> </li> <li> <p>Download model weights    Download from Google Drive and place in the <code>weights/</code> directory:</p> </li> <li><code>weights/soundscape-model.pt</code> \u2014 sound classification model</li> <li><code>weights/xgboost-model.pkl</code> \u2014 score prediction model</li> <li>Google Drive: Weights link</li> <li> <p>Ensure the model architecture matches <code>monsoon_biodiversity_common/config.py</code></p> </li> <li> <p>Build and run with Docker / Docker Compose</p> </li> <li>Build via script:      <pre><code>sh scripts/build_prod_image.sh\n</code></pre></li> <li>Or build with Docker directly:      <pre><code>docker build -t prod-bio-service -f docker/Dockerfile .\n</code></pre></li> <li>Start with Docker Compose:      <pre><code>docker compose -f docker/docker-compose-prod.yaml up -d\n</code></pre></li> <li> <p>Check containers:      <pre><code>docker ps\ndocker compose -f docker/docker-compose-prod.yaml ps\n</code></pre></p> </li> <li> <p>Verify logs (JSON sending and daily detections)</p> </li> <li>Follow container logs:      <pre><code>docker logs -f prod-bio-service\n</code></pre></li> <li>Inspect detailed log files inside the container:      <pre><code>docker exec -it prod-bio-service bash\ntail -n 200 -f /app/logs/audio_inference.log   # file monitoring &amp; segment inference\ntail -n 200 -f /app/logs/daily_report.log      # daily aggregation &amp; API sending\n</code></pre></li> <li>Expected entries:<ul> <li>Saved JSON: lines with \"[SAVE] JSON saved to\"</li> <li>Successful API send: lines with \"[API] Prediction sent\"</li> <li>Daily aggregation: lines with \"[DATE] Generating report for:\" and device/file counts</li> <li>Real-time processing: lines with \"[NEW FILE]\", \"[INFER] Processing\", and \"[DB] Added  detections\""}]}